<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/grayscale.css" rel="stylesheet">

</head>

<body id="page-top">

  <!-- Topic bias -->
  <section id="topic-bias" class="projects-section bg-light">
    <div class="container">

      <div class='inter-text text-black-50 mb-0 text-center'>
        <h1>Measuring the difference: a machine learning model to detect linguistic and topic bias</h1>
      </div>

       <p class='inter-text text-black-50 mb-0' align='justify'>We already know that there exists a difference in terms of numbers between male and female biographies in Wikipedia. We also found an example where a non-neutral language and stereotypes are used to describe a famous female personality. Now, the question is: Are these issues a generality in Wikipedia or did we tried really hard to find some examples?
        </p>

      <div class='inter-text text-black-50 mb-0 text-center'>
        <h1>Women are beautiful, Men are offensive</h1>
        <h3>According to Wikipedia</h3>
      </div>

      <p class='inter-text text-black-50 mb-0' align='justify'>In order to quantify these differences we trained a <strong>machine learning model</strong>. This model takes as input different words appearing in the overview of the biographies and aims to predict the gender of the person being described. The idea behind it is that if both genders are described in the same way, the model wonâ€™t be able to predict better than random (it is 50% in a balanced dataset). On the other hand, if the model can do better, it means there is a <strong>pattern to learn</strong> from the data and that actually, the bias is there.
      </p>

      <p class='inter-text text-black-50 mb-0' align='justify'>In this first part we perform the analysis using only the <strong>adjectives</strong>. We focus on this part of speech since it is used to perform descriptions and therefore is more prone to <strong>introduce subjectivity</strong> in the text. 
      </p>

      <p class='inter-text text-black-50 mb-0' align='justify'>Moreover, we perform the analysis on the <strong>overviews</strong> instead of the whole biography since it summarizes the main information about the person.  
      </p>


      <div class="row justify-content-center no-gutters mb-5 mb-lg-0">
        <div class="col-lg-5">
          <div class="bg-light-grey text-center h-100 project">
            <div class="d-flex h-100">
              <div class="project-text w-100 my-auto text-center text-lg-left">
                <h4 class="text-black" style="font-size: 1.6rem">How does the model really work? </h4>
                  <p class="mb-0 text-black" align="justify" style="font-size: 1.1rem"> We use a logistic regression model that takes as input a bag of words and outputs the probability that the encoded overview belongs to a woman. The encoding is done as a bag of words, it means a binary vector that signals if a word is present in the text or not. The vocabulary is build with the set of 100 most common words for men and the top 100 for women.</p>
                <hr class="d-none d-lg-block mb-0 ml-0">
              </div>
            </div>
          </div>
      </div>
      <div class="col-lg-1"></div>
      <div class="col-lg-5">
        <div>
            <div class="bg-grey text-center h-100 project">
              <div class="d-flex h-100">
                <div class="project-text w-100 my-auto text-center text-lg-left">
                  <h4 class="text-white" style="font-size: 1.6rem">How did the model perform?</h4>
                    <p class="mb-0 text-white" align="justify" style="font-size: 1.1rem">54.7% of the observations were correctly predicted, confirming that the bias does exist. </p>
                  <hr class="d-none d-lg-block mb-0 ml-0">
                </div>
              </div>
            </div>
        </div>
        <div>
          <div class="bg-grey text-center h-100 project">
            <div class="d-flex h-100">
              <div class="project-text w-100 my-auto text-center text-lg-left">
                <h4 class="text-white" style="font-size: 1.6rem">Is it a robust result?</h4>
                <p class="mb-0 text-white" align="justify" style="font-size: 1.1rem">It is. After 50 iterations, the model's results variated only 0.001.</p>
                <hr class="d-none d-lg-block mb-0 ml-0">
              </div>
            </div>
          </div>
        </div>
        <div>
          <div class="bg-grey text-center h-100 project">
            <div class="d-flex h-100">
              <div class="project-text w-100 my-auto text-center text-lg-left">
                <h4 class="text-white" style="font-size: 1.6rem">Is there the same bias for every field of occupation?</h4>
                <p class="mb-0 text-white" align="justify" style="font-size: 1.1rem">For the military and religion fields the confidence interval of the predictions is much widder and we can't afirm the existance of a bias.</p>
                <hr class="d-none d-lg-block mb-0 ml-0">
              </div>
            </div>
          </div>
        </div>
    </div>
  </div>


  </div>
</div>
</div>


      <!-- How did our model perform -->

      <!-- Is it a general result -->

      <!-- Is it true for every field of occupation -->


    </div>
  </section>

</body>

</html>
